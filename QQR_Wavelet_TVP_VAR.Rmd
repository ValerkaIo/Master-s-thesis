
```{r}
library(readxl)
df_f = read.csv('Волатильность_индекс.csv')
df_f$Date <- as.Date(df_f$Date)
df_f = df_f[(df_f$Date >= as.Date('2015-11-01')) & (df_f$Date < as.Date('2025-02-01')),]
head(df_f)
```

## Реализация QQR в R

```{r}
# Загрузка библиотек
library(rgl)
library(quantreg)
library(plot3D)
library(fields)
```

```{r quantiles}
### Определение квантилей и подготовка матрицы для результатов
quantiles <- seq(0.1, 0.9, by = 0.1)
nq <- length(quantiles)
beta_matrix <- matrix(0, nq, nq)
```

Сетка квантилей распределена от 0.1 до 0.9 с шагом 0.1.

```{r QQR_analysis}
compute_beta <- function(x, y, tau_x, tau_y, h = 0.05){
  # Вычисление значния x, соответствующего квантилю tau_x
  x_quantile_val <- quantile(x, tau_x)
  # Наблюдениям, близким к x_quantile_val, присваиваются большие веса.
  # Параметр h будет контролировать ширину окна: чем меньше h, тем меньше область вокруг квантиля.
  # Веса нормализуются, чтобы их сумма равнялась 1.
  kernel_weights <- dnorm((x - x_quantile_val) / h)
  kernel_weights <- kernel_weights / sum(kernel_weights)
  # Далее оценка квантильной регрессии для квантиля tau_y зависимой переменной.
  # Наблюдения взвешиваются: больший вес имеют данные, где x близок к своему квантилю tau_x.
  fit <- rq(y ~ x, tau = tau_y, weights = kernel_weights)
  # Возвращается коэффициент при x, который показывает, как x влияет на квантиль tau_y переменной y в окрестности квантиля tau_x для x.
  return(coef(fit)[2])}
```

Теперь перейдем к эмпирическому результату по QQR:

```{r QQR_results}
name = 'Price_USDC_LogReturn'

X_t = na.omit(df_f[, c(paste0(name), "VYTIndex")])$VYTIndex
Y_t = na.omit(df_f[, c(paste0(name), "VYTIndex")])[[name]]

# Вычисляем коэффициенты бета для каждой пары квантилей
for (i in 1:nq) {
  for (j in 1:nq) {
    beta_matrix[i, j] <- compute_beta(X_t, Y_t, quantiles[i], quantiles[j])
    cat("Обработка квантиля X:", quantiles[i], "и квантиля Y:", quantiles[j], "\n")
  }
}

# Подготовка данных для визуализации
x <- quantiles
y <- quantiles
z <- beta_matrix
# Создаем сетку координат
x_grid <- matrix(rep(x, each = length(y)), nrow = length(x))
y_grid <- matrix(rep(y, length(x)), nrow = length(x))

# Задаем цвета
jet.colors <- colorRampPalette(c("#00007F", "blue", "#007FFF", "cyan", "#7FFF7F", "yellow", "#FF7F00", "red", "#7F0000"))

library(plotly)

# От синего (отрицательные), через зеленый и желтый, к красному (положительные)
jet.colors <- colorRampPalette(c(
  "#00007F", "blue", "#007FFF", "cyan", "#7FFF7F", 
  "yellow", "#FF7F00", "red", "#7F0000"
))

n_colors <- 100  # Гладкая градация

plot_ly(
  x = ~quantiles,
  y = ~quantiles,
  z = ~beta_matrix,
  type = "surface",
  colorscale = list(
    list(0, "#00007F"),
    list(0.14, "blue"),
    list(0.28, "#007FFF"),
    list(0.42, "cyan"),
    list(0.57, "#7FFF7F"),
    list(0.71, "yellow"),
    list(0.85, "#FF7F00"),
    list(1, "red")
  ),
  showscale = TRUE
) %>%
layout(
  # title = list(
  #   text = "<b>3D тепловая карта коэффициентов бета QQR</b>",
  #   font = list(size = 20, family = "Arial")
  # ),
  scene = list(
    xaxis = list(
      title = "<b>VYTIndex</b>",
      gridcolor = "rgb(220,220,220)",
      tickformat = ".2f"
    ),
    yaxis = list(
      title = paste0("<b>" , gsub("Price_|Log|Return", "", name), "volatility</b>"),
      # title = paste0("<b>" , "SOL_volatility", "</b>"),
      gridcolor = "rgb(220,220,220)",
      tickformat = ".2f"
    ),
    zaxis = list(
      title = "<b>β</b>",
      gridcolor = "rgb(220,220,220)",
      tickformat = ".2f"
    ),
    camera = list(
      eye = list(x = 1.4, y = -1.4, z = 0.7)
    ),
    aspectratio = list(x = 1, y = 1, z = 0.7)
  ),
  margin = list(l = 60, r = 60, b = 60, t = 90),
  font = list(family = "Arial", size = 13)
) %>%
colorbar(
  title = "<b>β</b>",
  len = 0.7,
  tickformat = ".2f"
)

```


Эта визуализация представляет трехмерную поверхность, где:
- Ось X: квантили независимой переменной $X_t$ ($\tau_x$)
- Ось Y: квантили зависимой переменной $Y_t$ ($\tau_y$)
- Ось Z: оценки коэффициентов $\beta^{\tau_y}(x_{\tau_x})$
- Цвет: дополнительно кодирует значения коэффициентов от синего (низкие значения) до красного (высокие значения)

Ниже представлен код для проведения анализа вейвлет-когерентности в квантилях:

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Вейвлет когерентность

```{r}
library(WaveletComp)  # Пакет для вейвлет-анализа
library(dplyr)
library(lubridate)
library(ggplot2)
library(forecast)
library(gridExtra)
```


```{r wavelet_transform}
# Вейвлет-преобразование для индекса неопределенности VyTIndex
wt_uncertainty <- analyze.wavelet(df_f, "VYTIndex", 
                                 loess.span = 0,       # Без сглаживания тренда
                                 dt = 1,               # Шаг по времени (1 день)
                                 dj = 1/12,            # Шаг по масштабу
                                 lowerPeriod = 7,      # Минимальный масштаб (16 дней)
                                 upperPeriod = 365,    # Максимальный масштаб (512 дней)
                                 make.pval = TRUE,     # Расчет p-значений
                                 n.sim = 100)          # Количество симуляций
wt.image(wt_uncertainty, color.key = "quantile", n.levels = 100,
         main = "Вейвлет-спектр индекса неопределенности",
         timelab = "Время", periodlab = "Период (дни)")


# Вейвлет-преобразование для биткоина
wt_btc <- analyze.wavelet(df_f, "Price_BTC_LogReturn", 
                         loess.span = 0, 
                         dt = 1, dj = 1/12, 
                         lowerPeriod = 16, 
                         upperPeriod = 365,
                         make.pval = TRUE, 
                         n.sim = 100)
wt.image(wt_btc, color.key = "quantile", n.levels = 100,
         main = "Вейвлет-спектр доходности биткоина",
         timelab = "Время", periodlab = "Период (дни)")
```


```{r wavelet_results_upd}
library(biwavelet)
library(tidyr)

# 1. Обработка данных ------------------------------------------------------
name <- "Price_USDC_LogReturn"

# Преобразование даты и очистка данных
df_f <- df_f %>%
  mutate(date = as.Date(Date)) %>%
  drop_na(all_of(c(paste0(name), "VYTIndex", "date")))

# 2. Подготовка временных рядов -------------------------------------------
t <- seq_along(df_f$date)
time_series1 <- cbind(t, df_f$VYTIndex)
time_series2 <- cbind(t, df_f[[name]])

# 3. Расчет когерентности -------------------------------------------------
wtc_result <- wtc(
  time_series1,
  time_series2,
  pad = TRUE,
  dj = 1/12,
  s0 = 2 * (t[2] - t[1]),
  mother = "morlet",
  param = 6, # Значение для вейвлета Морле
  sig.level = 0.95,
  nrands = 100,
  quiet = FALSE
)

# 4. Визуалка (с try для ошибок) -----------------------------------
par(mar = c(5, 4, 4, 5) + 0.1)

tryCatch({
  plot(wtc_result,
       plot.cb = TRUE,
       plot.phase = TRUE,
       main = paste0("Wavelet Coherence: VYTIndex vs " , gsub("Price_|Log", "", name)),
       xlab = "",
       ylab = "Период (Дни)",
       xaxt = "n")
  
  # 5. Кастомизация осей --------------------------------------------------
  n_labels <- 7
  time_points <- floor(seq(1, length(t), length.out = n_labels))
  time_labels <- format(df_f$date[time_points], "%b\n%Y")
  
  # Фикс для последнего элемента
  if (time_points[length(time_points)] > length(t)) {
    time_points[length(time_points)] <- length(t)
  }
  
  axis(1, 
       at = time_points,
       labels = time_labels,
       las = 1,
       cex.axis = 0.8)
  
  # 6. Добавление сетки  ---------------------------------------------------
  abline(v = time_points, 
         col = "lightgray", 
         lty = "dotted")
  
  grid(nx = NA, ny = NULL, col = "gray90", lty = "dotted")
  
}, error = function(e) {
  message("Ошибка при построении графика: ", e$message)
})
```

Интерпретация карты вейвлет-когерентности:
На карте вейвлет-когерентности цвет показывает силу взаимосвязи (от синего - слабая, до желтого - сильная), а стрелки указывают на фазовые соотношения:
	•	Стрелки вправо (→): ряды в фазе (синхронное движение);
	•	Стрелки влево (←): ряды в противофазе (противоположное движение)
	•	Стрелки вниз (↓): индекс неопределенности опережает доходность
	•	Стрелки вверх (↑): доходность опережает индекс неопределенности


```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(quantreg, tseries, ggplot2, dplyr)
library(tseries)
```

# TVP-VAR connectedness

```{r}
# Load required libraries
library(remotes)
library(ConnectednessApproach)
library(knitr)
library(zoo)

# Function to perform TVP-VAR connectedness analysis with column selection
perform_tvp_var_analysis <- function(data, selected_columns, lag = 1, forecast_horizon = 100, 
                               window_size = 200, kappa1 = 0.99, kappa2 = 0.99) {
  
  # Print the available columns for reference
  cat("Available columns in the dataframe:\n")
  print(names(data))
  
  # Validate selected columns
  if (!all(selected_columns %in% names(data))) {
    stop("Some selected columns do not exist in the dataframe")
  }
  
  # Create a subset with selected columns
  data_subset <- data[, selected_columns, drop = FALSE]
  
  # Convert to zoo object
  data_zoo <- read.zoo(data_subset, header = TRUE, format = "%Y-%m-%d")
  
  # Display the selected data
  cat("\nSelected data for analysis:\n")
  print(head(data_zoo))
  
  # Define frequency partition
  partition <- c(pi+0.00001, pi/5, 0)
  
  # Perform TVP-VAR connectedness analysis
  cat("\nPerforming TVP-VAR connectedness analysis...\n")
  dca <- ConnectednessApproach(data_zoo, 
                              model = "TVP-VAR",
                              connectedness = "Frequency",
                              nlag = lag,
                              nfore = forecast_horizon,
                              window.size = window_size,
                              VAR_config = list(TVPVAR = list(kappa1 = kappa1, 
                                                          kappa2 = kappa2,
                                                          prior = "BayesPrior")),
                              Connectedness_config = list(
                                FrequencyConnectedness = list(partition = partition,
                                                            generalized = TRUE, 
                                                            scenario = "ABS")
                              ))
  
  # Return the analysis results
  return(dca)
}
```



```{r}
selected_cols <- c("Date", "Price_BTC_LogReturn", "Price_XRP_LogReturn", "Price_BNB_LogReturn", "Price_Cardano_LogReturn", "Price_Dogecoin_LogReturn",
                   "Price_ETH_LogReturn", "Price_Solana_LogReturn", "Price_STETH_LogReturn", "Price_USDt_LogReturn", "Price_USDC_LogReturn", "VYTIndex")


dca_results <- perform_tvp_var_analysis(drop_na(df_f), selected_cols)

kable(dca_results$TABLE)
PlotTCI(dca_results, ylim=c(0,100))
PlotNPDC(dca_results, ylim=c(-30,30))
PlotINF(dca_results, ylim=c(0,100))
PlotNetwork(dca_results, method="NPDC")
```





```{r}
# Удаляем "Price_" и "Log" из названий колонок

colnames(df_f) <- ifelse(
  colnames(df_f) == "Date",
  "Date",
  paste0(gsub("Price_|Log|Return", "", colnames(df_f)), "volatility")
)
colnames(df_f)[12:12] <- "VYTIndex"

# Обновляем список выбранных колонок
selected_cols <- c("Date", "BTC_volatility", "XRP_volatility", "BNB_volatility", "Cardano_volatility", "Dogecoin_volatility",
                   "ETH_volatility", "Solana_volatility", "STETH_volatility", "USDt_volatility", "USDC_volatility", "VYTIndex")

# Дальше твой анализ
dca_results <- perform_tvp_var_analysis(drop_na(df_f), selected_cols)
kable(dca_results$TABLE)
PlotTCI(dca_results, ylim=c(0,100))
PlotNPDC(dca_results, ylim=c(-30,30))
PlotINF(dca_results, ylim=c(0,100))
PlotNetwork(dca_results, method="NPDC")

PlotNetwork(
  dca = dca_results,
  method = "NPDC",
  threshold = 0.1,  # Уменьшение порога с 0.25 до 0.05
  name_length = 15,
  vertex.size = 12,
  edge.width = 2,
  edge.curved = 0.2
)

```

